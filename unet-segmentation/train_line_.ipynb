{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import albumentations as albu"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Инициализация TensorBoard\n",
    "writer = SummaryWriter(log_dir=\"runs/segmentation_experiment\")"
   ],
   "id": "b1a33909fa2d277f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATSET_NAME = r\"...\" # Путь к папке с датасетом\n",
    "\n",
    "X_TRAIN_DIR = f\"{DATSET_NAME}/Train\"\n",
    "Y_TRAIN_DIR = f\"{DATSET_NAME}/Trainannot\"\n",
    "\n",
    "X_VALID_DIR = f\"{DATSET_NAME}/Validation\"\n",
    "Y_VALID_DIR = f\"{DATSET_NAME}/Validationannot\"\n",
    "\n",
    "X_TEST_DIR = f\"{DATSET_NAME}/Validation\"\n",
    "Y_TEST_DIR = f\"{DATSET_NAME}/Validationannot\"\n",
    "\n",
    "LABEL_COLORS_FILE = f\"{DATSET_NAME}/label_colors.txt\""
   ],
   "id": "6839be00daaf4bfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Определяем классы и их цвета\n",
    "CLASSES = [\"background\", \"drop\"]\n",
    "colors_imshow = {\n",
    "    \"background\": np.array([0, 0, 0]),       # Черный цвет для фона\n",
    "    \"drop\": np.array([0, 0, 255]),           # Синий цвет для капли\n",
    "}\n",
    "\n",
    "\n",
    "ENCODER ='resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'softmax2d'\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "INIT_LR = 0.0005\n",
    "\n",
    "INFER_WIDTH = 512\n",
    "INFER_HEIGHT = 512\n",
    "\n",
    "loss = smp.utils.losses.DiceLoss()\n"
   ],
   "id": "1b531d0866a0ded1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _convert_multichannel2singlechannel(mc_mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    Преобразует многоканальную маску в одноканальное изображение с цветами для каждого класса.\n",
    "    Также вычисляет процентное соотношение каждого класса.\n",
    "    \"\"\"\n",
    "    sc_mask = np.zeros((mc_mask[0].shape[0], mc_mask[0].shape[1], 3), dtype=np.uint8)\n",
    "    square_ratios = {}\n",
    "\n",
    "    for i, singlechannel_mask in enumerate(mc_mask):\n",
    "        cls = CLASSES[i]\n",
    "        singlechannel_mask = singlechannel_mask.squeeze()\n",
    "\n",
    "        # Вычисляем процентное соотношение каждого класса\n",
    "        square_ratios[cls] = singlechannel_mask.sum() / singlechannel_mask.size\n",
    "\n",
    "        # Добавляем цвет для каждого класса\n",
    "        sc_mask += np.multiply.outer(singlechannel_mask > 0, colors_imshow[cls]).astype(np.uint8)\n",
    "\n",
    "    # Формируем заголовок с процентным соотношением классов\n",
    "    title = \"Процентное соотношение классов:\\n\" + \"\\n\".join([f\"{cls}: {square_ratios[cls] * 100:.1f}%\" for cls in CLASSES])\n",
    "    return sc_mask, title\n",
    "\n",
    "\n",
    "def visualize_multichennel_mask(img: np.ndarray, multichennel_mask: np.ndarray):\n",
    "    \"\"\"\n",
    "    Визуализирует изображение и соответствующую маску.\n",
    "    \"\"\"\n",
    "    # Создаем график с двумя подграфиками\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Отображаем исходное изображение\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Исходное изображение\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Преобразуем многоканальную маску и отображаем ее\n",
    "    multichennel_mask = multichennel_mask.transpose(2, 0, 1)\n",
    "    mask_to_show, title = _convert_multichannel2singlechannel(multichennel_mask)\n",
    "    axes[1].imshow(mask_to_show)\n",
    "    axes[1].set_title(\"Маска\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    # Добавляем заголовок с процентным соотношением классов\n",
    "    plt.suptitle(title, fontsize=12, y=0.95)\n",
    "\n",
    "    # Добавляем легенду для классов\n",
    "    legend_elements = [plt.Line2D([0], [0], marker=\"o\", color=\"w\", markerfacecolor=color / 255, markersize=10, label=cls)\n",
    "                      for cls, color in colors_imshow.items()]\n",
    "    fig.legend(handles=legend_elements, loc=\"lower center\", ncol=len(CLASSES), bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "2b6275e6555d212f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        masks_dir,\n",
    "        augmentation=None,\n",
    "        preprocessing=None\n",
    "    ):\n",
    "        self.images_paths = glob(f\"{images_dir}/*\")\n",
    "        self.masks_paths = glob(f\"{masks_dir}/*\")\n",
    "\n",
    "        self.cls_colors = self._get_classes_colors(LABEL_COLORS_FILE)\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def _get_classes_colors(self, label_colors_dir):\n",
    "        cls_colors = {}\n",
    "        with open(label_colors_dir) as file:\n",
    "            while line := file.readline():\n",
    "                R, G, B, label = line.rstrip().split()\n",
    "                cls_colors[label] = np.array([B, G, R], dtype=np.uint8)\n",
    "\n",
    "        keyorder = CLASSES\n",
    "        cls_colors_ordered = {}\n",
    "        for k in keyorder:\n",
    "            if k in cls_colors:\n",
    "                cls_colors_ordered[k] = cls_colors[k]\n",
    "            elif k == \"background\":\n",
    "                cls_colors_ordered[k] = np.array([0, 0, 0], dtype=np.uint8)\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected label {k}, cls colors: {cls_colors}\")\n",
    "\n",
    "        return cls_colors_ordered\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.imread(self.images_paths[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(self.masks_paths[i])\n",
    "        masks = [cv2.inRange(mask, color, color) for color in self.cls_colors.values()]\n",
    "        masks = [(m > 0).astype(\"float32\") for m in masks]\n",
    "        mask = np.stack(masks, axis=-1).astype(\"float\")\n",
    "\n",
    "        # Применяем аугментации\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        # Применяем предобработку\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)"
   ],
   "id": "5b5e4a2b5fedf06d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = Dataset(X_TRAIN_DIR, Y_TRAIN_DIR)\n",
    "image, mask = dataset[np.random.randint(len(dataset))]\n",
    "visualize_multichennel_mask(image, mask)"
   ],
   "id": "36e5de0c87a0d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        \n",
    "        albu.LongestMaxSize(max_size=INFER_HEIGHT),\n",
    "        albu.PadIfNeeded(min_height=int(INFER_HEIGHT*1.1), min_width=int(INFER_WIDTH*1.1), border_mode=2),\n",
    "        albu.RandomCrop(height=INFER_HEIGHT, width=INFER_WIDTH),\n",
    "        \n",
    "        # Размытия и шумы\n",
    "        albu.OneOf([\n",
    "            albu.MotionBlur(blur_limit=7, p=0.3),\n",
    "            albu.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "            albu.GlassBlur(sigma=0.7, max_delta=3, iterations=2, p=0.3)\n",
    "        ], p=0.7),\n",
    "        \n",
    "        albu.OneOf([\n",
    "            albu.GaussNoise(p=0.3), \n",
    "            albu.ISONoise(\n",
    "                color_shift=(0.01, 0.05),\n",
    "                intensity=(0.1, 0.5),\n",
    "                p=0.3\n",
    "            ),\n",
    "            albu.ImageCompression(p=0.3) \n",
    "        ], p=0.7),\n",
    "        \n",
    "        # Цветовые искажения\n",
    "        albu.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.2, 0.2),\n",
    "            contrast_limit=(-0.2, 0.2),\n",
    "            p=0.5\n",
    "        ),\n",
    "        albu.HueSaturationValue(\n",
    "            hue_shift_limit=10,\n",
    "            sat_shift_limit=20,\n",
    "            val_shift_limit=10,\n",
    "            p=0.5\n",
    "        )\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.LongestMaxSize(max_size=INFER_HEIGHT),\n",
    "        albu.PadIfNeeded(\n",
    "            min_height=INFER_HEIGHT,\n",
    "            min_width=INFER_WIDTH,\n",
    "            border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        albu.CenterCrop(height=INFER_HEIGHT, width=INFER_WIDTH)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    # Осуществит стартовую нормализацию данных согласно своим значениям или готовым для imagenet\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ],
   "id": "f67d0e20281c4e9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "augmented_dataset = Dataset(\n",
    "    X_TRAIN_DIR, \n",
    "    Y_TRAIN_DIR, \n",
    "    augmentation=get_training_augmentation()\n",
    ")\n",
    "\n",
    "# same image with different random transforms\n",
    "indx = np.random.randint(len(augmented_dataset))\n",
    "\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[indx]\n",
    "    visualize_multichennel_mask(image, mask)"
   ],
   "id": "547bc8be65fc2f82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "augmented_dataset = Dataset(\n",
    "    X_VALID_DIR, \n",
    "    Y_VALID_DIR, \n",
    "    augmentation=get_validation_augmentation()\n",
    ")\n",
    "\n",
    "indx = np.random.randint(len(augmented_dataset))\n",
    "\n",
    "image, mask = augmented_dataset[indx]\n",
    "visualize_multichennel_mask(image, mask)"
   ],
   "id": "1d73c16d9777027b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ],
   "id": "428b7720b2f6e60e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = Dataset(\n",
    "    X_TRAIN_DIR, \n",
    "    Y_TRAIN_DIR, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    X_VALID_DIR, \n",
    "    Y_VALID_DIR, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ],
   "id": "d83f0f1976a4a4bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = [\n",
    "    utils.metrics.Fscore(),\n",
    "    utils.metrics.IoU(),\n",
    "    utils.metrics.Precision(),\n",
    "    utils.metrics.Recall()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=INIT_LR),\n",
    "])\n",
    "\n",
    "# Scheduler без параметра verbose\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',  # Максимизируем IoU\n",
    "    factor=0.5,  # Уменьшаем LR в 2 раза\n",
    "    patience=5   # Ждём 5 эпох без улучшения\n",
    ")\n"
   ],
   "id": "2198c61e416c8837",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Инициализация эпох обучения и валидации\n",
    "train_epoch = utils.train.TrainEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ],
   "id": "2e1ac486c28c8b88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Цикл обучения\n",
    "max_score = 0\n",
    "loss_logs = {\"train\": [], \"val\": []}\n",
    "metric_logs = {\"train\": {\"iou\": [], \"fscore\": [], \"precision\": [], \"recall\": []},\n",
    "               \"val\": {\"iou\": [], \"fscore\": [], \"precision\": [], \"recall\": []}}\n",
    "\n",
    "for i in range(0, EPOCHS):\n",
    "    print(f'\\nEpoch: {i}')\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    train_loss, train_fscore, train_iou, train_precision, train_recall = list(train_logs.values())\n",
    "    \n",
    "    # Логируем тренировочные метрики\n",
    "    loss_logs[\"train\"].append(train_loss)\n",
    "    metric_logs[\"train\"][\"iou\"].append(train_iou)\n",
    "    metric_logs[\"train\"][\"fscore\"].append(train_fscore)\n",
    "    metric_logs[\"train\"][\"precision\"].append(train_precision)\n",
    "    metric_logs[\"train\"][\"recall\"].append(train_recall)\n",
    "    \n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    val_loss, val_fscore, val_iou, val_precision, val_recall = list(valid_logs.values())\n",
    "    \n",
    "    # Логируем валидационные метрики\n",
    "    loss_logs[\"val\"].append(val_loss)\n",
    "    metric_logs[\"val\"][\"iou\"].append(val_iou)\n",
    "    metric_logs[\"val\"][\"fscore\"].append(val_fscore)\n",
    "    metric_logs[\"val\"][\"precision\"].append(val_precision)\n",
    "    metric_logs[\"val\"][\"recall\"].append(val_recall)\n",
    "    \n",
    "    # Логирование в TensorBoard\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, i)\n",
    "    writer.add_scalar(\"Loss/val\", val_loss, i)\n",
    "    writer.add_scalar(\"IoU/train\", train_iou, i)\n",
    "    writer.add_scalar(\"IoU/val\", val_iou, i)\n",
    "    writer.add_scalar(\"Fscore/train\", train_fscore, i)\n",
    "    writer.add_scalar(\"Fscore/val\", val_fscore, i)\n",
    "    writer.add_scalar(\"Precision/train\", train_precision, i)\n",
    "    writer.add_scalar(\"Precision/val\", val_precision, i)\n",
    "    writer.add_scalar(\"Recall/train\", train_recall, i)\n",
    "    writer.add_scalar(\"Recall/val\", val_recall, i)\n",
    "    writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], i)\n",
    "    \n",
    "    # Сохранение модели при улучшении IoU\n",
    "    if max_score < val_iou:\n",
    "        max_score = val_iou\n",
    "        torch.save(model.state_dict(), 'models/best_model_new.pth')\n",
    "        print('Model saved (.pth)!')\n",
    "        \n",
    "        # Трассировка модели (с предупреждением о фиксированном размере)\n",
    "        try:\n",
    "            trace_image = torch.randn(BATCH_SIZE, 3, INFER_HEIGHT, INFER_WIDTH)\n",
    "            traced_model = torch.jit.trace(model, trace_image.to(DEVICE))\n",
    "            torch.jit.save(traced_model, 'models/best_model_new.pt')\n",
    "            print('Traced model saved (.pt)!')\n",
    "        except Exception as e:\n",
    "            print(f\"Tracing failed: {e}. Skipping JIT save.\")\n",
    "    \n",
    "    # Сохранение чекпоинта каждые 10 эпох\n",
    "    if i % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'models/checkpoint_epoch_{i}.pth')\n",
    "        print(f'Checkpoint saved for epoch {i}!')\n",
    "    \n",
    "    # Обновление learning rate с помощью scheduler\n",
    "    scheduler.step(val_iou)\n",
    "    print(f\"Current LR: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# Закрываем TensorBoard\n",
    "writer.close()\n"
   ],
   "id": "e7b9bdbf3ac56d6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
